{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHhiystgWRx_"
      },
      "source": [
        "### Assignment-03\n",
        "\n",
        "- Formulate three original hypotheses based on the raw US Accidents dataset, and perform appropriate statistical hypothesis tests to validate each one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "9tc7nzKpWO73"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 7728394 entries, 0 to 7728393\n",
            "Data columns (total 46 columns):\n",
            " #   Column                 Dtype  \n",
            "---  ------                 -----  \n",
            " 0   ID                     object \n",
            " 1   Source                 object \n",
            " 2   Severity               int64  \n",
            " 3   Start_Time             object \n",
            " 4   End_Time               object \n",
            " 5   Start_Lat              float64\n",
            " 6   Start_Lng              float64\n",
            " 7   End_Lat                float64\n",
            " 8   End_Lng                float64\n",
            " 9   Distance(mi)           float64\n",
            " 10  Description            object \n",
            " 11  Street                 object \n",
            " 12  City                   object \n",
            " 13  County                 object \n",
            " 14  State                  object \n",
            " 15  Zipcode                object \n",
            " 16  Country                object \n",
            " 17  Timezone               object \n",
            " 18  Airport_Code           object \n",
            " 19  Weather_Timestamp      object \n",
            " 20  Temperature(F)         float64\n",
            " 21  Wind_Chill(F)          float64\n",
            " 22  Humidity(%)            float64\n",
            " 23  Pressure(in)           float64\n",
            " 24  Visibility(mi)         float64\n",
            " 25  Wind_Direction         object \n",
            " 26  Wind_Speed(mph)        float64\n",
            " 27  Precipitation(in)      float64\n",
            " 28  Weather_Condition      object \n",
            " 29  Amenity                bool   \n",
            " 30  Bump                   bool   \n",
            " 31  Crossing               bool   \n",
            " 32  Give_Way               bool   \n",
            " 33  Junction               bool   \n",
            " 34  No_Exit                bool   \n",
            " 35  Railway                bool   \n",
            " 36  Roundabout             bool   \n",
            " 37  Station                bool   \n",
            " 38  Stop                   bool   \n",
            " 39  Traffic_Calming        bool   \n",
            " 40  Traffic_Signal         bool   \n",
            " 41  Turning_Loop           bool   \n",
            " 42  Sunrise_Sunset         object \n",
            " 43  Civil_Twilight         object \n",
            " 44  Nautical_Twilight      object \n",
            " 45  Astronomical_Twilight  object \n",
            "dtypes: bool(13), float64(12), int64(1), object(20)\n",
            "memory usage: 2.0+ GB\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "df=pd.read_csv(\"US_Accidents_March23.csv\")\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def permutation_test(x, y, n_permutations=1000, alternative=\"greater\", seed=42):\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    x = np.array(x)\n",
        "    y = np.array(y)\n",
        "\n",
        "    observed_diff = x.mean() - y.mean()\n",
        "    combined = np.concatenate([x, y])\n",
        "\n",
        "    perm_diffs = []\n",
        "    for _ in range(n_permutations):\n",
        "        np.random.shuffle(combined)\n",
        "        perm_x = combined[:len(x)]\n",
        "        perm_y = combined[len(x):]\n",
        "        perm_diffs.append(perm_x.mean() - perm_y.mean())\n",
        "\n",
        "    perm_diffs = np.array(perm_diffs)\n",
        "\n",
        "    if alternative == \"greater\":\n",
        "        p_value = np.mean(perm_diffs >= observed_diff)\n",
        "    elif alternative == \"less\":\n",
        "        p_value = np.mean(perm_diffs <= observed_diff)\n",
        "    else:\n",
        "        p_value = np.mean(np.abs(perm_diffs) >= abs(observed_diff))\n",
        "\n",
        "    return observed_diff, p_value\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Hypothesis 1: \n",
        "Nighttime accidents are more severe than daytime accidents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean severity difference (Night - Day): 0.009673063529520132\n",
            "p-value: 0.0\n",
            "Hypothesis supported\n"
          ]
        }
      ],
      "source": [
        "day = df[df[\"Sunrise_Sunset\"] == \"Day\"][\"Severity\"].dropna()\n",
        "night = df[df[\"Sunrise_Sunset\"] == \"Night\"][\"Severity\"].dropna()\n",
        "diff, p_value = permutation_test(\n",
        "    night, day,\n",
        "    n_permutations=100,\n",
        "    alternative=\"greater\"\n",
        ")\n",
        "print(\"Mean severity difference (Night - Day):\", diff)\n",
        "print(\"p-value:\", p_value)\n",
        "print(\n",
        "    \"Hypothesis supported\" if p_value < 0.05\n",
        "    else \"Hypothesis not supported\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Hypothesis 2: \n",
        "Accidents near traffic signals have higher severity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean severity difference (Signal - No Signal): -0.1440243048909231\n",
            "p-value: 1.0\n",
            "Hypothesis not supported\n"
          ]
        }
      ],
      "source": [
        "signal = df[df[\"Traffic_Signal\"] == True][\"Severity\"].dropna()\n",
        "no_signal = df[df[\"Traffic_Signal\"] == False][\"Severity\"].dropna()\n",
        "diff, p_value = permutation_test(\n",
        "    signal, no_signal,\n",
        "    n_permutations=100,\n",
        "    alternative=\"greater\"\n",
        ")\n",
        "\n",
        "print(\"Mean severity difference (Signal - No Signal):\", diff)\n",
        "print(\"p-value:\", p_value)\n",
        "print(\n",
        "    \"Hypothesis supported\" if p_value < 0.05\n",
        "    else \"Hypothesis not supported\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Hypothesis 3: \n",
        "Low visibility increases accident severity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean severity difference (Low - High visibility): 0.01439966021159833\n",
            "p-value: 0.0\n",
            "Hypothesis supported\n"
          ]
        }
      ],
      "source": [
        "low_vis = df[df[\"Visibility(mi)\"] < 3][\"Severity\"].dropna()\n",
        "high_vis = df[df[\"Visibility(mi)\"] >= 3][\"Severity\"].dropna()\n",
        "diff, p_value = permutation_test(\n",
        "    low_vis, high_vis,\n",
        "    n_permutations=100,\n",
        "    alternative=\"greater\"\n",
        ")\n",
        "\n",
        "print(\"Mean severity difference (Low - High visibility):\", diff)\n",
        "print(\"p-value:\", p_value)\n",
        "print(\n",
        "    \"Hypothesis supported\" if p_value < 0.05\n",
        "    else \"Hypothesis not supported\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "995d4251"
      },
      "source": [
        "# Task\n",
        "Analyze the US Accidents dataset by loading and exploring it, then formulate and statistically test three hypotheses: one on the impact of weather conditions on accident severity/frequency, one on the impact of time of day on accident frequency/severity, and one on the influence of road features on accident occurrence/severity. Finally, summarize and interpret the results of these tests."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1f73c533"
      },
      "source": [
        "## Load and Explore Data\n",
        "\n",
        "### Subtask:\n",
        "Load the US Accidents dataset and perform initial exploration to understand its structure, columns, and potential areas for hypothesis formulation. This includes displaying head, info, and descriptive statistics.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5e8dae95"
      },
      "source": [
        "**Reasoning**:\n",
        "First, I need to import the pandas library to work with DataFrames. Then, I will load the 'US_Accidents_Dec21_updated.csv' dataset into a pandas DataFrame named `df` as per the instructions. After loading, I will display the first 5 rows, print a concise summary of the DataFrame (info), and generate descriptive statistics to fulfill the subtask requirements.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "fd6f9667",
        "outputId": "48ca97e9-689c-4b3c-9bfa-bcf82f8e78e9"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'US_Accidents_Dec21_updated.csv'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4244021195.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Load the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'US_Accidents_Dec21_updated.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Display the first 5 rows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'US_Accidents_Dec21_updated.csv'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('US_Accidents_Dec21_updated.csv')\n",
        "\n",
        "# Display the first 5 rows\n",
        "print(\"First 5 rows of the DataFrame:\")\n",
        "print(df.head())\n",
        "\n",
        "# Print a concise summary of the DataFrame\n",
        "print(\"\\nDataFrame Information:\")\n",
        "df.info()\n",
        "\n",
        "# Generate descriptive statistics for numerical columns\n",
        "print(\"\\nDescriptive Statistics:\")\n",
        "print(df.describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00f45530"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed because the specified CSV file was not found. I need to update the file path to the correct location where the dataset is stored, which is typically in the `/kaggle/input/` directory in a Colab environment for a Kaggle dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "b5dec67f",
        "outputId": "822c0f3c-255a-4f4f-bc2c-6260d095b8cf"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/kaggle/input/us-accidents-dec21-updated/US_Accidents_Dec21_updated.csv'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2979314899.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Load the dataset with the corrected path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/kaggle/input/us-accidents-dec21-updated/US_Accidents_Dec21_updated.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Display the first 5 rows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/us-accidents-dec21-updated/US_Accidents_Dec21_updated.csv'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset with the corrected path\n",
        "df = pd.read_csv('/kaggle/input/us-accidents-dec21-updated/US_Accidents_Dec21_updated.csv')\n",
        "\n",
        "# Display the first 5 rows\n",
        "print(\"First 5 rows of the DataFrame:\")\n",
        "print(df.head())\n",
        "\n",
        "# Print a concise summary of the DataFrame\n",
        "print(\"\\nDataFrame Information:\")\n",
        "df.info()\n",
        "\n",
        "# Generate descriptive statistics for numerical columns\n",
        "print(\"\\nDescriptive Statistics:\")\n",
        "print(df.describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3a82319"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempts to load the dataset failed due to a FileNotFoundError, indicating the dataset was not present in the specified paths. To resolve this in a Colab environment for a Kaggle dataset, I will add steps to install the Kaggle API client, download the dataset from Kaggle, and then unzip it into the working directory before loading it with pandas.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "841cc848"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Install Kaggle API client if not already installed\n",
        "!pip install kaggle --quiet\n",
        "\n",
        "# --- IMPORTANT: Manual step for Kaggle API key --- #\n",
        "# For this to work, you need to upload your kaggle.json file.\n",
        "# Steps:\n",
        "# 1. Go to Kaggle, click on your profile picture -> 'Account'.\n",
        "# 2. Scroll down to 'API' section and click 'Create New API Token'. This downloads 'kaggle.json'.\n",
        "# 3. In Colab, run the following (if not already done):\n",
        "#    from google.colab import files\n",
        "#    files.upload() # Then upload kaggle.json\n",
        "# 4. Move kaggle.json to the correct directory and set permissions:\n",
        "#    !mkdir -p ~/.kaggle\n",
        "#    !mv kaggle.json ~/.kaggle/\n",
        "#    !chmod 600 ~/.kaggle/kaggle.json\n",
        "# --- End of manual step instructions --- #\n",
        "\n",
        "# Define the Kaggle dataset details\n",
        "kaggle_dataset_owner = 'sourabhshah1'\n",
        "kaggle_dataset_name = 'us-accidents-dec21-updated'\n",
        "zip_file_name = f'{kaggle_dataset_name}.zip'\n",
        "csv_file_name = 'US_Accidents_Dec21_updated.csv'\n",
        "\n",
        "# Download the dataset from Kaggle\n",
        "print(f\"Downloading {kaggle_dataset_owner}/{kaggle_dataset_name} dataset...\")\n",
        "!kaggle datasets download -d {kaggle_dataset_owner}/{kaggle_dataset_name}\n",
        "\n",
        "# Unzip the downloaded file. The -o flag overwrites if already exists.\n",
        "print(f\"Unzipping {zip_file_name}...\")\n",
        "!unzip -o {zip_file_name} -d .\n",
        "\n",
        "# Load the dataset from the unzipped CSV file (now in the current directory)\n",
        "df = pd.read_csv(csv_file_name)\n",
        "\n",
        "# Display the first 5 rows\n",
        "print(\"\\nFirst 5 rows of the DataFrame:\")\n",
        "print(df.head())\n",
        "\n",
        "# Print a concise summary of the DataFrame\n",
        "print(\"\\nDataFrame Information:\")\n",
        "df.info()\n",
        "\n",
        "# Generate descriptive statistics for numerical columns\n",
        "print(\"\\nDescriptive Statistics:\")\n",
        "print(df.describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "959Cmq2Wtd8A"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73becbe0"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed because the Kaggle API key was not properly configured, leading to a `KeyError: 'username'` during the dataset download attempt, and subsequently a `FileNotFoundError` for the zip and CSV files. The core issue is the manual step of uploading and configuring `kaggle.json`. I will re-provide the same code block, as its logic is correct for downloading a Kaggle dataset, but add a prominent print statement to re-emphasize the critical manual steps required to set up the Kaggle API key before executing the download command.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 703
        },
        "id": "d3e1bea9",
        "outputId": "a21b650b-9640-44ed-9171-869c030803ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "*** ENSURE KAGGLE API KEY (kaggle.json) IS UPLOADED AND CONFIGURED AS PER INSTRUCTIONS ABOVE ***\n",
            "\n",
            "Downloading sourabhshah1/us-accidents-dec21-updated dataset...\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/kaggle\", line 10, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/kaggle/cli.py\", line 68, in main\n",
            "    out = args.func(**command_args)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/kaggle/api/kaggle_api_extended.py\", line 1741, in dataset_download_cli\n",
            "    with self.build_kaggle_client() as kaggle:\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/kaggle/api/kaggle_api_extended.py\", line 688, in build_kaggle_client\n",
            "    username=self.config_values['username'],\n",
            "             ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\n",
            "KeyError: 'username'\n",
            "Unzipping us-accidents-dec21-updated.zip...\n",
            "unzip:  cannot find or open us-accidents-dec21-updated.zip, us-accidents-dec21-updated.zip.zip or us-accidents-dec21-updated.zip.ZIP.\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'US_Accidents_Dec21_updated.csv'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2383806513.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;31m# Load the dataset from the unzipped CSV file (now in the current directory)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_file_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;31m# Display the first 5 rows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'US_Accidents_Dec21_updated.csv'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Install Kaggle API client if not already installed\n",
        "!pip install kaggle --quiet\n",
        "\n",
        "# --- IMPORTANT: MANUAL KAGGLE API KEY SETUP REQUIRED --- #\n",
        "# YOU MUST UPLOAD YOUR KAGGLE.JSON AND CONFIGURE IT.\n",
        "# Please ensure you have completed the following steps before proceeding:\n",
        "# 1. Go to Kaggle, click on your profile picture -> 'Account'.\n",
        "# 2. Scroll down to 'API' section and click 'Create New API Token'. This downloads 'kaggle.json'.\n",
        "# 3. In Colab, run the following commands (if not already done) to upload and configure your API key:\n",
        "#    from google.colab import files\n",
        "#    files.upload() # Then upload kaggle.json\n",
        "#    !mkdir -p ~/.kaggle\n",
        "#    !mv kaggle.json ~/.kaggle/\n",
        "#    !chmod 600 ~/.kaggle/kaggle.json\n",
        "# Without these steps, the dataset download will fail.\n",
        "print(\"\\n*** ENSURE KAGGLE API KEY (kaggle.json) IS UPLOADED AND CONFIGURED AS PER INSTRUCTIONS ABOVE ***\\n\")\n",
        "# --- END OF MANUAL STEP INSTRUCTIONS --- #\n",
        "\n",
        "# Define the Kaggle dataset details\n",
        "kaggle_dataset_owner = 'sourabhshah1'\n",
        "kaggle_dataset_name = 'us-accidents-dec21-updated'\n",
        "zip_file_name = f'{kaggle_dataset_name}.zip'\n",
        "csv_file_name = 'US_Accidents_Dec21_updated.csv'\n",
        "\n",
        "# Download the dataset from Kaggle\n",
        "print(f\"Downloading {kaggle_dataset_owner}/{kaggle_dataset_name} dataset...\")\n",
        "!kaggle datasets download -d {kaggle_dataset_owner}/{kaggle_dataset_name}\n",
        "\n",
        "# Unzip the downloaded file. The -o flag overwrites if already exists.\n",
        "print(f\"Unzipping {zip_file_name}...\")\n",
        "!unzip -o {zip_file_name} -d .\n",
        "\n",
        "# Load the dataset from the unzipped CSV file (now in the current directory)\n",
        "df = pd.read_csv(csv_file_name)\n",
        "\n",
        "# Display the first 5 rows\n",
        "print(\"\\nFirst 5 rows of the DataFrame:\")\n",
        "print(df.head())\n",
        "\n",
        "# Print a concise summary of the DataFrame\n",
        "print(\"\\nDataFrame Information:\")\n",
        "df.info()\n",
        "\n",
        "# Generate descriptive statistics for numerical columns\n",
        "print(\"\\nDescriptive Statistics:\")\n",
        "print(df.describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4e1267c5"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed because the Kaggle API key was not properly configured, leading to a `KeyError: 'username'` during the dataset download attempt. The critical manual steps for uploading and configuring `kaggle.json` were present as comments but not actively executed. To fix this, I will incorporate the necessary Colab-specific commands to handle the `kaggle.json` upload and configuration directly into the code block, preceding the dataset download, along with clear user instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7eb6a8f0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from google.colab import files # Import files for uploading\n",
        "\n",
        "# Install Kaggle API client if not already installed\n",
        "!pip install kaggle --quiet\n",
        "\n",
        "# --- IMPORTANT: KAGGLE API KEY SETUP --- #\n",
        "# YOU MUST UPLOAD YOUR KAGGLE.JSON AND CONFIGURE IT.\n",
        "# This part requires user interaction in the Colab environment.\n",
        "print(\"\\n*** PLEASE FOLLOW THE PROMPT BELOW TO UPLOAD YOUR KAGGLE.JSON FILE ***\")\n",
        "print(\"Go to Kaggle, click on your profile picture -> 'Account'.\")\n",
        "print(\"Scroll down to 'API' section and click 'Create New API Token'. This downloads 'kaggle.json'.\")\n",
        "print(\"Then, when prompted by the 'files.upload()' dialog, select your downloaded 'kaggle.json'.\\n\")\n",
        "\n",
        "# Upload kaggle.json\n",
        "files.upload() # This will open a file selection dialog\n",
        "\n",
        "# Move kaggle.json to the correct directory and set permissions\n",
        "!mkdir -p ~/.kaggle\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "print(\"\\n*** Kaggle API key configured successfully! ***\\n\")\n",
        "# --- END OF KAGGLE API KEY SETUP --- #\n",
        "\n",
        "# Define the Kaggle dataset details\n",
        "kaggle_dataset_owner = 'sourabhshah1'\n",
        "kaggle_dataset_name = 'us-accidents-dec21-updated'\n",
        "zip_file_name = f'{kaggle_dataset_name}.zip'\n",
        "csv_file_name = 'US_Accidents_Dec21_updated.csv'\n",
        "\n",
        "# Download the dataset from Kaggle\n",
        "print(f\"Downloading {kaggle_dataset_owner}/{kaggle_dataset_name} dataset...\")\n",
        "!kaggle datasets download -d {kaggle_dataset_owner}/{kaggle_dataset_name}\n",
        "\n",
        "# Unzip the downloaded file. The -o flag overwrites if already exists.\n",
        "print(f\"Unzipping {zip_file_name}...\")\n",
        "!unzip -o {zip_file_name} -d .\n",
        "\n",
        "# Load the dataset from the unzipped CSV file (now in the current directory)\n",
        "df = pd.read_csv(csv_file_name)\n",
        "\n",
        "# Display the first 5 rows\n",
        "print(\"\\nFirst 5 rows of the DataFrame:\")\n",
        "print(df.head())\n",
        "\n",
        "# Print a concise summary of the DataFrame\n",
        "print(\"\\nDataFrame Information:\")\n",
        "df.info()\n",
        "\n",
        "# Generate descriptive statistics for numerical columns\n",
        "print(\"\\nDescriptive Statistics:\")\n",
        "print(df.describe())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
